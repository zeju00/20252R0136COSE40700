import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import seaborn as sns
import joblib
import optuna  # Optuna ì¶”ê°€

# ==========================================
# 1. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ (ê¸°ì¡´ ì½”ë“œ ë™ì¼)
# ==========================================
print("--- [1/6] ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ ---")

try:
    #df = pd.read_csv("dataset/ml_dataset.csv")
    #df = pd.read_csv("dataset/ml_dataset_hidden_size.csv")
    df = pd.read_csv("dataset/ml_dataset_memory_size.csv")
except FileNotFoundError:
    print("ì˜¤ë¥˜: íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    exit()

output_columns = [
    'fwd_avg_w', 'fwd_max_w', 'fwd_min_w',
    'comm_avg_w', 'comm_max_w', 'comm_min_w',
    'bwd_avg_w', 'bwd_max_w', 'bwd_min_w'
]

input_features = [
    'network_bandwidth', 'gpu_freq', 'cpu_freq',
    'active_gpu_cores', 'active_cpu_cores', 'transformer_blocks'
]

df_cleaned = df.dropna(subset=output_columns)
df_grouped = df_cleaned.groupby(input_features)[output_columns].mean().reset_index()

X = df_grouped[input_features]
Y = df_grouped[output_columns]

# Train/Test Split
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

# ìŠ¤ì¼€ì¼ë§
x_scaler = StandardScaler()
y_scaler = StandardScaler()

X_train_scaled = x_scaler.fit_transform(X_train)
Y_train_scaled = y_scaler.fit_transform(Y_train)
X_test_scaled = x_scaler.transform(X_test)
Y_test_values = Y_test.values # í‰ê°€ìš© ì›ë³¸ ê°’

joblib.dump(x_scaler, 'model/dnn/dnn_x_scaler.pkl')
joblib.dump(y_scaler, 'model/dnn/dnn_y_scaler.pkl')

# í…ì„œ ë³€í™˜
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
X_train_tensor = torch.FloatTensor(X_train_scaled).to(device)
Y_train_tensor = torch.FloatTensor(Y_train_scaled).to(device)
X_test_tensor = torch.FloatTensor(X_test_scaled).to(device)

input_dim = len(input_features)
output_dim = len(output_columns)

print(f"Data Setup Complete. Device: {device}")

# ==========================================
# 2. Optuna Objective í•¨ìˆ˜ ì •ì˜
# ==========================================
def build_model(trial, input_dim, output_dim):
    """Optuna Trialì— ë”°ë¼ ë™ì ìœ¼ë¡œ ëª¨ë¸ êµ¬ì¡°ë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜"""
    n_layers = trial.suggest_int("n_layers", 1, 4) # ì€ë‹‰ì¸µ 1~4ê°œ íƒìƒ‰
    layers = []
    
    in_features = input_dim
    for i in range(n_layers):
        out_features = trial.suggest_int(f"n_units_l{i}", 32, 256) # ë…¸ë“œ ìˆ˜ 32~256 íƒìƒ‰
        layers.append(nn.Linear(in_features, out_features))
        layers.append(nn.ReLU())
        
        # Dropout ì ìš© (ê³¼ì í•© ë°©ì§€)
        p = trial.suggest_float(f"dropout_l{i}", 0.0, 0.5)
        layers.append(nn.Dropout(p))
        
        in_features = out_features
        
    layers.append(nn.Linear(in_features, output_dim))
    return nn.Sequential(*layers)

def objective(trial):
    # 1. ëª¨ë¸ ìƒì„±
    model = build_model(trial, input_dim, output_dim).to(device)
    
    # 2. í•˜ì´í¼íŒŒë¼ë¯¸í„° ì œì•ˆ
    lr = trial.suggest_float("lr", 1e-5, 1e-1, log=True)
    batch_size = trial.suggest_categorical("batch_size", [16, 32, 64])
    optimizer_name = trial.suggest_categorical("optimizer", ["Adam", "RMSprop"])
    
    # Optimizer ì„¤ì •
    if optimizer_name == "Adam":
        optimizer = optim.Adam(model.parameters(), lr=lr)
    else:
        optimizer = optim.RMSprop(model.parameters(), lr=lr)
        
    criterion = nn.L1Loss() # MAE Loss

    # DataLoader ìƒì„± (Batch Sizeê°€ íŠœë‹ ëŒ€ìƒì´ë¯€ë¡œ ì—¬ê¸°ì„œ ìƒì„±)
    train_dataset = TensorDataset(X_train_tensor, Y_train_tensor)
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    
    # 3. í•™ìŠµ ë£¨í”„ (Pruning í¬í•¨)
    model.train()
    for epoch in range(50): # íŠœë‹ìš© Epochì€ ì¡°ê¸ˆ ì§§ê²Œ ì„¤ì • (ì†ë„ ìœ„í•´)
        for inputs, labels in train_loader:
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
        
        # Validation (Pruning ìš©)
        model.eval()
        with torch.no_grad():
            outputs_val = model(X_test_tensor)
            val_loss = criterion(outputs_val, torch.FloatTensor(y_scaler.transform(Y_test)).to(device)).item()
        model.train()
        
        # Optunaì—ê²Œ í˜„ì¬ ì„±ëŠ¥ ë³´ê³ 
        trial.report(val_loss, epoch)
        
        # ê°€ë§ ì—†ìœ¼ë©´ ì¡°ê¸° ì¢…ë£Œ (Pruning)
        if trial.should_prune():
            raise optuna.exceptions.TrialPruned()
            
    return val_loss

# ==========================================
# 3. Optuna ìµœì í™” ì‹¤í–‰
# ==========================================
print("\n--- [2/6] Optuna Hyperparameter Tuning ---")
study = optuna.create_study(direction="minimize", pruner=optuna.pruners.MedianPruner())
study.optimize(objective, n_trials=100) # 50ë²ˆ ì‹œë„ (ì‹œê°„ì— ë”°ë¼ ì¡°ì ˆ)

print("\nğŸ† Best Trial:")
print(f"  Value (Scaled MAE): {study.best_value:.4f}")
print("  Params: ")
for key, value in study.best_params.items():
    print(f"    {key}: {value}")

# ==========================================
# 4. ìµœì  íŒŒë¼ë¯¸í„°ë¡œ ìµœì¢… ì¬í•™ìŠµ (Full Epochs)
# ==========================================
print("\n--- [3/6] Retraining with Best Parameters ---")

best_params = study.best_params
final_epochs = 200 # ìµœì¢… í•™ìŠµì€ ì¶©ë¶„íˆ ê¸¸ê²Œ

# Best Model êµ¬ì¡° ì¬ìƒì„±
# ì£¼ì˜: build_modelì€ trial ê°ì²´ë¥¼ í•„ìš”ë¡œ í•˜ë¯€ë¡œ, best_params ë”•ì…”ë„ˆë¦¬ë¥¼ í™œìš©í•´ ìˆ˜ë™ìœ¼ë¡œ êµ¬ì„±í•˜ê±°ë‚˜
# Optunaì˜ FixedTrialì„ ì“¸ ìˆ˜ ìˆì§€ë§Œ, ì—¬ê¸°ì„  ì§ê´€ì ìœ¼ë¡œ ë‹¤ì‹œ êµ¬ì„±í•©ë‹ˆë‹¤.

layers = []
in_features = input_dim
for i in range(best_params['n_layers']):
    out_features = best_params[f"n_units_l{i}"]
    layers.append(nn.Linear(in_features, out_features))
    layers.append(nn.ReLU())
    p = best_params[f"dropout_l{i}"]
    layers.append(nn.Dropout(p))
    in_features = out_features
layers.append(nn.Linear(in_features, output_dim))

best_model = nn.Sequential(*layers).to(device)

# Optimizer ì¬ì„¤ì •
lr = best_params['lr']
if best_params['optimizer'] == "Adam":
    optimizer = optim.Adam(best_model.parameters(), lr=lr)
else:
    optimizer = optim.RMSprop(best_model.parameters(), lr=lr)

criterion = nn.L1Loss()
train_dataset = TensorDataset(X_train_tensor, Y_train_tensor)
train_loader = DataLoader(train_dataset, batch_size=best_params['batch_size'], shuffle=True)

# ìµœì¢… í•™ìŠµ
best_model.train()
loss_history = []

for epoch in range(final_epochs):
    epoch_loss = 0
    for inputs, labels in train_loader:
        optimizer.zero_grad()
        outputs = best_model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        epoch_loss += loss.item()
    
    if (epoch + 1) % 20 == 0:
        print(f"  Epoch [{epoch+1}/{final_epochs}], Loss: {epoch_loss/len(train_loader):.4f}")

# ëª¨ë¸ ì €ì¥
torch.save(best_model.state_dict(), "model/dnn/best_dnn_model.pth")
print("âœ… Best Optuna Model Saved.")

# ==========================================
# 5. ìµœì¢… í‰ê°€ (ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼)
# ==========================================
print("\n--- [4/6] Final Evaluation ---")
best_model.eval()
with torch.no_grad():
    Y_pred_scaled = best_model(X_test_tensor).cpu().numpy()

# ìŠ¤ì¼€ì¼ ë³µì›
Y_pred = y_scaler.inverse_transform(Y_pred_scaled)

# ì¢…í•© ì§€í‘œ
mae = mean_absolute_error(Y_test_values, Y_pred)
rmse = np.sqrt(mean_squared_error(Y_test_values, Y_pred))
mape = mean_absolute_percentage_error(Y_test_values, Y_pred)

print(f"  [Overall Performance]")
print(f"  - MAE  : {mae:.4f}")
print(f"  - RMSE : {rmse:.4f}")
print(f"  - MAPE : {mape*100:.2f} (%)")

# ì»¬ëŸ¼ë³„ ìƒì„¸ ì§€í‘œ
mae_per_col = []
for i, col in enumerate(output_columns):
    c_mae = mean_absolute_error(Y_test_values[:, i], Y_pred[:, i])
    mae_per_col.append(c_mae)

# ==========================================
# 6. ì‹œê°í™” (ê¸°ì¡´ ì½”ë“œ ìœ ì§€)
# ==========================================
print("\n--- [5/6] Visualization ---")
plt.style.use('seaborn-v0_8-whitegrid')

# 1. MAE Bar Plot
plt.figure(figsize=(12, 6))
sns.barplot(x=mae_per_col, y=output_columns, hue=output_columns, palette='viridis', legend=False)
plt.title(f'MAE by Output (Optuna Tuned)\nBest Params: {best_params["n_layers"]} Layers, {best_params["optimizer"]}', fontsize=14)
plt.xlabel('MAE (Watt)')
plt.tight_layout()
plt.savefig("eval/dnn/dnn_optuna_mae.png", dpi=300)

# 2. Scatter Plot (Overall)
plt.figure(figsize=(10, 10))
y_true_flat = Y_test_values.flatten()
y_pred_flat = Y_pred.flatten()
plt.scatter(y_true_flat, y_pred_flat, alpha=0.2, s=10, color='darkblue')
min_val = min(y_true_flat.min(), y_pred_flat.min())
max_val = max(y_true_flat.max(), y_pred_flat.max())
plt.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2)
plt.title(f'Overall True vs Pred (Optuna)\nRMSE: {rmse:.4f}, MAE: {mae:.4f}', fontsize=15)
plt.tight_layout()
plt.savefig("eval/dnn/dnn_optuna_scatter.png", dpi=300)

print("âœ… Visualization saved.")

# ==========================================
# 7. CSV ì €ì¥
# ==========================================
eval_results = pd.DataFrame()
for i, col in enumerate(output_columns):
    eval_results[f'True_{col}'] = Y_test_values[:, i]
    eval_results[f'Pred_{col}'] = Y_pred[:, i]

eval_results.to_csv("eval/dnn/dnn_optuna_results.csv", index=False)
print("âœ… Results CSV saved.")